"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
ï¼ˆFaissä¿å­˜æ–¹æ³•ä¿®æ­£ç‰ˆï¼‰
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from dotenv import load_dotenv
import streamlit as st
import logging
import sys
import unicodedata
import gspread
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.schema import HumanMessage, AIMessage, Document
from langchain_openai import OpenAIEmbeddings
# ChromaDBã®ä»£ã‚ã‚Šã«Faissã‚’ä½¿ç”¨
from langchain_community.vectorstores import FAISS
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from typing import List
from sudachipy import tokenizer, dictionary
from langchain_community.agent_toolkits import SlackToolkit
from langchain.agents import AgentType, initialize_agent
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from docx import Document
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain import LLMChain
import datetime
import constants as ct
from pathlib import Path
from collections import Counter
from langchain.schema.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter as RegexTextSplitter
import pickle
import hashlib
import json
from oauth2client.service_account import ServiceAccountCredentials
from langchain_community.utilities import GoogleSearchAPIWrapper
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import traceback
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any, Dict, List, Optional, Union


# ============================================================================
# åŒç¾©èªè¾æ›¸ï¼ˆå¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µå¯èƒ½ï¼‰
# ============================================================================

SYNONYM_DICT = {
    "å—è³": ["å—è³æ­´", "è¡¨å½°", "ã‚¢ãƒ¯ãƒ¼ãƒ‰", "è³", "æ „èª‰"],
    "å®Ÿç¸¾": ["æˆæœ", "æ¥­ç¸¾", "çµæœ", "æˆç¸¾"],
    "ä¼šç¤¾": ["ä¼æ¥­", "æ³•äºº", "çµ„ç¹”", "äº‹æ¥­è€…"],
    "ç’°å¢ƒ": ["ã‚¨ã‚³", "ã‚°ãƒªãƒ¼ãƒ³", "ã‚µã‚¹ãƒ†ãƒŠãƒ–ãƒ«", "æŒç¶šå¯èƒ½"],
    "è£½å“": ["å•†å“", "ã‚µãƒ¼ãƒ“ã‚¹", "ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ"],
    "é¡§å®¢": ["ãŠå®¢æ§˜", "åˆ©ç”¨è€…", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"],
    "æ–™é‡‘": ["ä¾¡æ ¼", "è²»ç”¨", "ã‚³ã‚¹ãƒˆ", "å€¤æ®µ", "é‡‘é¡"],
    "é…é€": ["ç™ºé€", "é…é”", "ãŠå±Šã‘", "è¼¸é€"],
    "æ³¨æ–‡": ["ã‚ªãƒ¼ãƒ€ãƒ¼", "ç™ºæ³¨", "è³¼å…¥", "ç”³è¾¼"]
}

############################################################
# è¨­å®šé–¢é€£
############################################################
load_dotenv()


############################################################
# Faisså°‚ç”¨ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿é–¢æ•°
############################################################

def save_faiss_index(db, base_path):
    """
    Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é©åˆ‡ãªæ–¹æ³•ã§ä¿å­˜
    
    Args:
        db: FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        base_path: ä¿å­˜å…ˆã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
    """
    try:
        # Faisså°‚ç”¨ã®ä¿å­˜æ–¹æ³•ã‚’ä½¿ç”¨
        db.save_local(base_path)
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.info(f"âœ… Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜å®Œäº†: {base_path}")
        return True
    except Exception as e:
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.error(f"âŒ Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def load_faiss_index(base_path, embeddings):
    """
    Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é©åˆ‡ãªæ–¹æ³•ã§èª­ã¿è¾¼ã¿
    
    Args:
        base_path: èª­ã¿è¾¼ã¿å…ƒã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        embeddings: åŸ‹ã‚è¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        
    Returns:
        FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ ã¾ãŸã¯ None
    """
    try:
        # Faisså°‚ç”¨ã®èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’ä½¿ç”¨
        db = FAISS.load_local(base_path, embeddings, allow_dangerous_deserialization=True)
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.info(f"âœ… Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿å®Œäº†: {base_path}")
        return db
    except Exception as e:
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.warning(f"âš ï¸ Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None

def calculate_docs_hash(docs):
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œçŸ¥ç”¨ï¼‰
    
    Args:
        docs: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        
    Returns:
        ãƒãƒƒã‚·ãƒ¥å€¤æ–‡å­—åˆ—
    """
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
    content_str = ""
    for doc in docs:
        content_str += doc.page_content + str(doc.metadata)
    
    return hashlib.md5(content_str.encode('utf-8')).hexdigest()

def should_rebuild_index(base_path, docs):
    """
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†æ§‹ç¯‰ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
    
    Args:
        base_path: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        docs: ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        
    Returns:
        bool: å†æ§‹ç¯‰ãŒå¿…è¦ãªå ´åˆTrue
    """
    metadata_file = f"{base_path}_metadata.json"
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å†æ§‹ç¯‰
    if not os.path.exists(metadata_file):
        return True
    
    # Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å†æ§‹ç¯‰
    if not os.path.exists(f"{base_path}.faiss"):
        return True
    
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒã‚·ãƒ¥ã‚’æ¯”è¼ƒ
        current_hash = calculate_docs_hash(docs)
        stored_hash = metadata.get('docs_hash', '')
        
        if current_hash != stored_hash:
            logger = logging.getLogger(ct.LOGGER_NAME)
            logger.info("ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå¤‰æ›´ã•ã‚ŒãŸãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã—ã¾ã™")
            return True
        
        return False
        
    except Exception as e:
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.warning(f"âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€å†æ§‹ç¯‰ã—ã¾ã™: {e}")
        return True

def save_index_metadata(base_path, docs):
    """
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    
    Args:
        base_path: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        docs: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    """
    try:
        metadata_file = f"{base_path}_metadata.json"
        metadata = {
            'docs_hash': calculate_docs_hash(docs),
            'doc_count': len(docs),
            'created_at': datetime.datetime.now().isoformat(),
            'version': '1.0'
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {metadata_file}")
        
    except Exception as e:
        logger = logging.getLogger(ct.LOGGER_NAME)
        logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

############################################################
# é–¢æ•°å®šç¾©
############################################################
def create_base_vectorstore(db_name):
    """
    å…±é€šã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆå‡¦ç†
    
    Args:
        db_name: ãƒ™ã‚¯ãƒˆãƒ«DBã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
        
    Returns:
        FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ”¨ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆé–‹å§‹: {db_name}")
    
    docs_all = []
    
    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä½¿ã‚ãªã„å ´åˆã®å‡¦ç†
    if db_name == ct.DB_ALL_PATH:
        folders = os.listdir(ct.RAG_TOP_FOLDER_PATH)
        # ã€Œdataã€ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã®å„ãƒ•ã‚©ãƒ«ãƒ€åã«å¯¾ã—ã¦å‡¦ç†
        for folder_path in folders:
            if folder_path.startswith("."):
                continue
            # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
            add_docs(f"{ct.RAG_TOP_FOLDER_PATH}/{folder_path}", docs_all)
    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä½¿ã†å ´åˆã®å‡¦ç†
    else:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åã«å¯¾å¿œã—ãŸã€RAGåŒ–å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ç¾¤ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—
        folder_path = ct.DB_NAMES[db_name]
        # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        add_docs(folder_path, docs_all)

    # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
    for doc in docs_all:
        doc.page_content = adjust_string(doc.page_content)
        for key in doc.metadata:
            doc.metadata[key] = adjust_string(doc.metadata[key])
    
    text_splitter = CharacterTextSplitter(
        chunk_size=ct.CHUNK_SIZE,
        chunk_overlap=ct.CHUNK_OVERLAP,
        separator="\n",
    )
    splitted_docs = text_splitter.split_documents(docs_all)
    
    # âœ… ãƒãƒ£ãƒ³ã‚¯å…ˆé ­ã«ãƒ¡ã‚¿æƒ…å ±ã‚’ä»˜åŠ ï¼ˆretrieverã¨åŒã˜å‡¦ç†ï¼‰
    for doc in splitted_docs:
        file_name = doc.metadata.get("file_name", "ä¸æ˜")
        category = doc.metadata.get("category", "ä¸æ˜")
        heading = doc.metadata.get("first_heading", "")
        keywords_str = doc.metadata.get("top_keywords", "") 

        prefix = f"ã€ã‚«ãƒ†ã‚´ãƒª: {category}ã€‘ã€ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}ã€‘"
        if heading:
            prefix += f"ã€è¦‹å‡ºã—: {heading}ã€‘"
        if keywords_str:  # â† ä¿®æ­£ï¼šæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨
            prefix += f"ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords_str}ã€‘"

        doc.page_content = prefix + "\n" + doc.page_content

    embeddings = OpenAIEmbeddings()

    # Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
    base_path = f"{db_name}_faiss"
    
    # å†æ§‹ç¯‰ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
    if should_rebuild_index(base_path, splitted_docs):
        logger.info(f"ğŸ”¨ Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ä¸­: {base_path}")
        db = FAISS.from_documents(splitted_docs, embeddings)
        
        # Faisså°‚ç”¨ã®ä¿å­˜æ–¹æ³•ã‚’ä½¿ç”¨
        if save_faiss_index(db, base_path):
            save_index_metadata(base_path, splitted_docs)
        
    else:
        # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        db = load_faiss_index(base_path, embeddings)
        if db is None:
            # èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯å†ä½œæˆ
            logger.info("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿å¤±æ•—ã€å†ä½œæˆã—ã¾ã™")
            db = FAISS.from_documents(splitted_docs, embeddings)
            if save_faiss_index(db, base_path):
                save_index_metadata(base_path, splitted_docs)
    
    logger.info(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆå®Œäº†: {db_name}")
    return db

def run_doc_chain_base(chain_name, param):
    """
    å…±é€šã®Docãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œå‡¦ç†
    
    Args:
        chain_name: ãƒã‚§ãƒ¼ãƒ³åï¼ˆä¾‹: "company", "service", "customer" ãªã©ï¼‰
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤
        
    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ”— {chain_name}ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œé–‹å§‹")
    
    try:
        # ãƒã‚§ãƒ¼ãƒ³åã‹ã‚‰å¯¾å¿œã™ã‚‹session_stateã®å±æ€§ã‚’å–å¾—
        # ä¾‹: chain_name="company" â†’ st.session_state.company_doc_chain
        chain_attr_name = f"{chain_name}_doc_chain"
        chain = getattr(st.session_state, chain_attr_name)
        
        # ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œ
        ai_msg = chain.invoke({
            "input": param,
            "chat_history": st.session_state.chat_history
        })
        
        # ä¼šè©±å±¥æ­´ã¸ã®è¿½åŠ 
        st.session_state.chat_history.extend([
            HumanMessage(content=param),
            AIMessage(content=ai_msg["answer"])
        ])
        
        logger.info(f"âœ… {chain_name}ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œå®Œäº†")
        return ai_msg["answer"]
        
    except Exception as e:
        logger.error(f"âŒ {chain_name}ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å®‰å…¨ã«å‡¦ç†ã‚’ç¶™ç¶š
        return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€{chain_name}ã«é–¢ã™ã‚‹æƒ…å ±ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def build_knowledge_vectorstore():
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰ï¼ˆFAISSç‰ˆï¼‰
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info("ğŸ§± ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰é–‹å§‹")
    
    try:
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰Q&Aãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        docs = load_qa_from_google_sheet(ct.GOOGLE_SHEET_URL)
        
        if not docs:
            logger.warning("âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return False
        
        # åŸ‹ã‚è¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        embeddings = OpenAIEmbeddings()
        
        # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        db = FAISS.from_documents(docs, embeddings)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜
        base_path = f"{ct.DB_KNOWLEDGE_PATH}_faiss"
        success = save_faiss_index(db, base_path)
        
        if success:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
            save_index_metadata(base_path, docs)
            logger.info(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰å®Œäº†: {len(docs)} docs â†’ {base_path}")
            return True
        else:
            logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«DBä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return False

def load_qa_from_google_sheet(sheet_url: str) -> List[Document]:
    """
    Googleã‚·ãƒ¼ãƒˆã‹ã‚‰Q&Aãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰
    
    Args:
        sheet_url: Googleã‚·ãƒ¼ãƒˆã®URL
        
    Returns:
        Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info("ğŸ” ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿é–‹å§‹")
    
    try:
        # Google Sheets APIèªè¨¼
        scope = [
            "https://spreadsheets.google.com/feeds", 
            "https://www.googleapis.com/auth/drive"
        ]
        
        # èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        auth_file_path = 'secrets/service_account.json'
        if not os.path.exists(auth_file_path):
            logger.error(f"âŒ èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {auth_file_path}")
            return []
        
        creds = ServiceAccountCredentials.from_json_keyfile_name(auth_file_path, scope)
        client = gspread.authorize(creds)
        
        # ã‚·ãƒ¼ãƒˆã‚’é–‹ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        sheet = client.open_by_url(sheet_url).sheet1
        rows = sheet.get_all_records()
        
        logger.info(f"ğŸ“Š ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ {len(rows)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {type(e).__name__} - {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return []

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
    docs = []
    for i, row in enumerate(rows):
        try:
            # å„ã‚«ãƒ©ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæŸ”è»Ÿã«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’å‡¦ç†ï¼‰
            q = row.get("è³ªå•", row.get("question", ""))
            a = row.get("å›ç­”", row.get("answer", ""))
            src = row.get("æ ¹æ‹ è³‡æ–™", row.get("source", ""))
            cat = row.get("å¯¾å¿œã‚«ãƒ†ã‚´ãƒª", row.get("category", ""))
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if not q or not a:
                logger.warning(f"âš ï¸ {i+1}è¡Œç›®: è³ªå•ã¾ãŸã¯å›ç­”ãŒç©ºã§ã™ (Q='{q}', A='{a}')")
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            meta = {
                "file_name": "GoogleSheet",
                "category": cat if cat else "ä¸€èˆ¬",
                "source": src if src else "ç¤¾å†…Q&A",
                "top_keywords": q,  # è³ªå•ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨
                "row_number": i + 1,
                "sheet_url": sheet_url
            }
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä½œæˆ
            content = f"Q: {q}\nA: {a}"
            if src:
                content += f"\næ ¹æ‹ : {src}"
            
            # Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            doc = Document(page_content=content, metadata=meta)
            docs.append(doc)
            
        except Exception as e:
            logger.warning(f"âš ï¸ {i+1}è¡Œç›®ã®ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {type(e).__name__} - {e}")
            continue
    
    logger.info(f"âœ… {len(docs)}ä»¶ã®æœ‰åŠ¹ãªDocumentã‚’ä½œæˆ")
    return docs

def search_knowledge(query: str, top_k: int = 3, score_threshold: float = 0.3):
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ï¼ˆFAISSç‰ˆï¼‰
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        top_k: å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°
        score_threshold: ã‚¹ã‚³ã‚¢ã®é–¾å€¤
        
    Returns:
        æ¤œç´¢çµæœã®Documentãƒªã‚¹ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ” ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢é–‹å§‹ï¼š'{query}'")
    
    try:
        # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å­˜åœ¨ç¢ºèª
        if not hasattr(st.session_state, 'knowledge_doc_chain'):
            logger.error("âŒ knowledge_doc_chainãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        # æ¤œç´¢å®Ÿè¡Œ
        retriever = st.session_state.knowledge_doc_chain.retriever
        results = retriever.get_relevant_documents(query)
        
        if not results:
            logger.info("ğŸ”¸ æ¤œç´¢çµæœï¼š0ä»¶")
            return []
        
        # ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆFAISSã®å ´åˆã¯ã‚¹ã‚³ã‚¢ãŒå–å¾—ã§ããªã„å ´åˆãŒã‚ã‚‹ãŸã‚æŸ”è»Ÿã«å‡¦ç†ï¼‰
        filtered = []
        for doc in results[:top_k]:  # ã¾ãšä¸Šä½top_kä»¶ã«çµã‚‹
            score = doc.metadata.get("score", 1.0)  # ã‚¹ã‚³ã‚¢ãŒãªã„å ´åˆã¯1.0ã¨ã™ã‚‹
            if score >= score_threshold:
                filtered.append(doc)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç©ºã®å ´åˆã¯ä¸Šä½çµæœã‚’ãã®ã¾ã¾è¿”ã™
        if not filtered and results:
            filtered = results[:top_k]
            logger.info(f"ğŸ”¸ ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾ŒãŒç©ºã®ãŸã‚ã€ä¸Šä½{len(filtered)}ä»¶ã‚’è¿”ã—ã¾ã™")
        
        logger.info(f"ğŸ”¸ æœ€çµ‚æ¤œç´¢çµæœ: {len(filtered)}ä»¶")
        return filtered
        
    except Exception as e:
        logger.error(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__} - {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return []

def safe_get_secret(key):
    """
    ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Streamlit Secretsã‹ã‚‰å®‰å…¨ã«å€¤ã‚’å–å¾—
    
    Args:
        key: ç’°å¢ƒå¤‰æ•°å
        
    Returns:
        å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å€¤ã€å­˜åœ¨ã—ãªã„å ´åˆã¯None
    """
        
    # ã¾ãšç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
    env_value = os.getenv(key)
    if env_value:
        return env_value
    
    # ç’°å¢ƒå¤‰æ•°ã«ãªã„å ´åˆã€Streamlit Secretsã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
    try:
        return st.secrets.get(key)
    except Exception:
        # secrets.tomlãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆ
        return None

def check_env_var_status(key):
    """
    ç’°å¢ƒå¤‰æ•°ã®è¨­å®šçŠ¶æ³ã‚’æ–‡å­—åˆ—ã§è¿”ã™ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    
    Args:
        key: ç’°å¢ƒå¤‰æ•°å
        
    Returns:
        "è¨­å®šæ¸ˆã¿" ã¾ãŸã¯ "æœªè¨­å®š"
    """
    return "è¨­å®šæ¸ˆã¿" if safe_get_secret(key) else "æœªè¨­å®š"

def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])

def clean_keyword(keyword): 
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ä¸æ­£ãªæ–‡å­—ã‚’é™¤å»
    """
    import re
    # ã‚¼ãƒ­å¹…æ–‡å­—ã‚„Markdownè¨˜å·ã‚’é™¤å»
    cleaned = re.sub(r'[\u200b\u200c\u200d\ufeffâ—â—‹â– â–¡â–¶â—‡â—†\.]', '', keyword)
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    cleaned = cleaned.strip()
    return cleaned

def expand_keywords_with_synonyms(keywords: List[str]) -> List[str]:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã«åŒç¾©èªã‚’è¿½åŠ ã—ã¦æ‹¡å¼µ
    
    Args:
        keywords: å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["å—è³æ­´"]ï¼‰
        
    Returns:
        åŒç¾©èªã‚’å«ã‚€æ‹¡å¼µã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["å—è³æ­´", "å—è³", "è¡¨å½°", "ã‚¢ãƒ¯ãƒ¼ãƒ‰", "è³", "æ „èª‰"]ï¼‰
    """
    # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚setã‚’ä½¿ç”¨
    expanded = set(keywords)
    
    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã¤ã„ã¦åŒç¾©èªã‚’æ¢ã™
    for keyword in keywords:
        # å®Œå…¨ä¸€è‡´ã®åŒç¾©èªã‚’è¿½åŠ 
        for base_word, synonyms in SYNONYM_DICT.items():
            if keyword == base_word:
                # base_wordã®åŒç¾©èªã‚’ã™ã¹ã¦è¿½åŠ 
                expanded.update(synonyms)
            elif keyword in synonyms:
                # keywordãŒåŒç¾©èªãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
                expanded.add(base_word)
                expanded.update(synonyms)
        
        # éƒ¨åˆ†ãƒãƒƒãƒã®åŒç¾©èªã‚’è¿½åŠ 
        for base_word, synonyms in SYNONYM_DICT.items():
            if base_word in keyword or keyword in base_word:
                expanded.add(base_word)
                expanded.update(synonyms)
    
    return list(expanded)

def check_flexible_keyword_match(query_keywords: List[str], doc_keywords: List[str]) -> tuple:
    """
    æŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
    
    Args:
        query_keywords: ã‚¯ã‚¨ãƒªã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        doc_keywords: æ–‡æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        
    Returns:
        (ãƒãƒƒãƒã—ãŸã‹ã©ã†ã‹, ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ, ãƒãƒƒãƒã‚¿ã‚¤ãƒ—)
    """
    matched_keywords = []  # ãƒãƒƒãƒã—ãŸçµ„ã¿åˆã‚ã›ã‚’è¨˜éŒ²
    match_types = []       # ãƒãƒƒãƒã®ç¨®é¡ã‚’è¨˜éŒ²
    
    # ã‚¯ã‚¨ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åŒç¾©èªã§æ‹¡å¼µ
    expanded_query_keywords = expand_keywords_with_synonyms(query_keywords)
    
    # æ‹¡å¼µã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ–‡æ›¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¯”è¼ƒ
    for query_kw in expanded_query_keywords:
        for doc_kw in doc_keywords:
            # 1. å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            if query_kw == doc_kw:
                matched_keywords.append(f"{query_kw}={doc_kw}")
                match_types.append("å®Œå…¨ä¸€è‡´")
                continue
            
            # 2. éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¯ã‚¨ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ–‡æ›¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹ï¼‰
            if query_kw in doc_kw:
                matched_keywords.append(f"{query_kw}âŠ†{doc_kw}")
                match_types.append("éƒ¨åˆ†ä¸€è‡´")
                continue
            
            # 3. é€†éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆæ–‡æ›¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¯ã‚¨ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹ï¼‰
            if doc_kw in query_kw:
                matched_keywords.append(f"{doc_kw}âŠ†{query_kw}")
                match_types.append("é€†éƒ¨åˆ†ä¸€è‡´")
                continue
    
    # ãƒãƒƒãƒã—ãŸã‹ã©ã†ã‹ã‚’åˆ¤å®š
    is_match = len(matched_keywords) > 0
    return is_match, matched_keywords, match_types

def filter_chunks_by_flexible_keywords(docs, query):
    """
    æŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚’ä½¿ã£ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    Args:
        docs: æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆï¼ˆDocumentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¯ã‚¨ãƒªã‹ã‚‰åè©ã‚’æŠ½å‡º
        tokenizer_obj = dictionary.Dictionary().create()
        mode = tokenizer.Tokenizer.SplitMode.C
        tokens = tokenizer_obj.tokenize(query, mode)
        query_nouns = [
            t.surface() 
            for t in tokens
            if "åè©" in t.part_of_speech() and len(t.surface()) > 1
        ]
        
        logger.info(f"ğŸ“ æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªåè©: {query_nouns}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å„æ–‡æ›¸ã¨ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
        filtered_docs = []
        for doc in docs:
            # æ–‡æ›¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
            top_keywords_str = doc.metadata.get("top_keywords", "")
            if top_keywords_str:
                top_keywords = [kw.strip() for kw in top_keywords_str.split(" / ") if kw.strip()]
                
                # æŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
                is_match, matched_keywords, match_types = check_flexible_keyword_match(
                    query_nouns, top_keywords
                )
                
                # ãƒãƒƒãƒã—ãŸå ´åˆã¯çµæœãƒªã‚¹ãƒˆã«è¿½åŠ 
                if is_match:
                    filtered_docs.append(doc)
                    logger.info(f"âœ… ãƒãƒƒãƒæˆåŠŸ:")
                    logger.info(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {doc.metadata.get('file_name', 'ä¸æ˜')}")
                    logger.info(f"   ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {matched_keywords}")
                    logger.info(f"   ãƒãƒƒãƒã‚¿ã‚¤ãƒ—: {set(match_types)}")
        
        logger.info(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ: {len(docs)} â†’ {len(filtered_docs)} ä»¶")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        if not filtered_docs:
            logger.info("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç©ºã®ãŸã‚ã€å…ƒã®docsã‚’è¿”ã—ã¾ã™")
            return docs  # å®‰å…¨ç­–ï¼šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç©ºã®å ´åˆã¯å…ƒã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        return filtered_docs
        
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return docs  # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å®‰å…¨ç­–ã¨ã—ã¦å…ƒã®ãƒªã‚¹ãƒˆã‚’è¿”ã™

def create_knowledge_rag_chain():
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆï¼ˆFAISSç‰ˆï¼‰
    
    Returns:
        RAGãƒã‚§ãƒ¼ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info("ğŸ”— ãƒŠãƒ¬ãƒƒã‚¸RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆé–‹å§‹")
    
    try:
        # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿
        embeddings = OpenAIEmbeddings()
        base_path = f"{ct.DB_KNOWLEDGE_PATH}_faiss"
        
        # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        db = load_faiss_index(base_path, embeddings)
        
        if db is None:
            logger.warning("âš ï¸ æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™")
            success = build_knowledge_vectorstore()
            if not success:
                logger.error("âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            # ä½œæˆå¾Œã«å†åº¦èª­ã¿è¾¼ã¿
            db = load_faiss_index(base_path, embeddings)
            if db is None:
                logger.error("âŒ ä½œæˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
        
        # Retrieverä½œæˆ
        retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})
        
        # RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
        question_answer_prompt = ChatPromptTemplate.from_messages([
            ("system", question_answer_template),
            ("human", "{input}")
        ])
        
        # ãƒã‚§ãƒ¼ãƒ³ä½œæˆ
        question_answer_chain = create_stuff_documents_chain(
            st.session_state.llm, 
            question_answer_prompt
        )
        
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)
        
        logger.info("âœ… ãƒŠãƒ¬ãƒƒã‚¸RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆå®Œäº†")
        return rag_chain
        
    except Exception as e:
        logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return None


def create_rag_chain(db_name):
    """
    å¼•æ•°ã¨ã—ã¦æ¸¡ã•ã‚ŒãŸDBå†…ã‚’å‚ç…§ã™ã‚‹RAGã®Chainã‚’ä½œæˆï¼ˆå…±é€šåŒ–ç‰ˆï¼‰

    Args:
        db_name: RAGåŒ–å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å
        
    Returns:
        RAGãƒã‚§ãƒ¼ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ”— RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆé–‹å§‹: {db_name}")
    
    # å…±é€šã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆå‡¦ç†ã‚’ä½¿ç”¨
    db = create_base_vectorstore(db_name)
    retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})

    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        st.session_state.llm, retriever, question_generator_prompt
    )

    question_answer_chain = create_stuff_documents_chain(st.session_state.llm, question_answer_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    logger.info(f"âœ… RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆå®Œäº†: {db_name}")
    return rag_chain


def create_retriever(db_name):
    """
    æŒ‡å®šã•ã‚ŒãŸDBãƒ‘ã‚¹ã«åŸºã¥ã„ã¦Retrieverã®ã¿ã‚’ä½œæˆï¼ˆå…±é€šåŒ–ç‰ˆï¼‰

    Args:
        db_name: ãƒ™ã‚¯ãƒˆãƒ«DBã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼ˆã¾ãŸã¯å®šç¾©åï¼‰

    Returns:
        LangChainã®Retrieverã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ” Retrieverä½œæˆé–‹å§‹: {db_name}")
    
    # å…±é€šã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆå‡¦ç†ã‚’ä½¿ç”¨
    db = create_base_vectorstore(db_name)

    retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})
    logger.info(f"âœ… Retrieverä½œæˆå®Œäº†: {db_name}")
    return retriever

def add_docs(folder_path, docs_all):
    """
    ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—

    Args:
        folder_path: ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        docs_all: å„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    """
    print(f"ğŸ“‚ èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€: {folder_path}")
    print(f"ğŸ“‚ ãƒ•ãƒ«ãƒ‘ã‚¹: {os.path.abspath(folder_path)}")

    files = os.listdir(folder_path)
    for file in files:
        file_path = os.path.join(folder_path, file)
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’å–å¾—
        file_extension = os.path.splitext(file)[1]
        # æƒ³å®šã—ã¦ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®å ´åˆã®ã¿èª­ã¿è¾¼ã‚€
        if file_extension in ct.SUPPORTED_EXTENSIONS:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã«åˆã£ãŸdata loaderã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            loader = ct.SUPPORTED_EXTENSIONS[file_extension](f"{folder_path}/{file}")
        else:
            continue

        docs = loader.load()
        p = Path(file_path)

        for doc in docs:
            content = doc.page_content

            # ğŸ“Œ åŸºæœ¬ãƒ¡ã‚¿æƒ…å ±
            doc.metadata["file_name"] = p.name
            doc.metadata["file_stem"] = p.stem
            doc.metadata["file_ext"] = p.suffix
            doc.metadata["file_path"] = str(p)
            doc.metadata["category"] = p.parent.name
            doc.metadata["file_mtime"] = datetime.datetime.fromtimestamp(p.stat().st_mtime).isoformat()
            doc.metadata["file_ctime"] = datetime.datetime.fromtimestamp(p.stat().st_ctime).isoformat()

            # ğŸ§‘â€ğŸ’» ä½œæˆè€…ï¼ˆdocxã®å ´åˆã®ã¿å–å¾—å¯èƒ½ãªå ´åˆã‚ã‚Šï¼‰
            if p.suffix == ".docx":
                try:
                    from docx import Document as DocxDocument
                    core_props = DocxDocument(p).core_properties
                    doc.metadata["file_author"] = core_props.author
                except Exception:
                    doc.metadata["file_author"] = "ä¸æ˜"
            else:
                doc.metadata["file_author"] = "ä¸æ˜"

            # ğŸ§© ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—
            section_titles = []
            for line in content.splitlines():
                if line.strip().startswith(("â– ", "â—", "â—‹", "ã€", "â–¶", "â—‡", "â—†")):
                    section_titles.append(line.strip())
            doc.metadata["section_titles"] = " / ".join(section_titles)
            doc.metadata["first_heading"] = section_titles[0] if section_titles else ""
            doc.metadata["section_count"] = len(section_titles)

            # ğŸ§  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆåè©ã®ã¿ï¼‰- ä¿®æ­£ç‰ˆ
            try:
                # å½¢æ…‹ç´ è§£æã®å®Ÿè¡Œ
                tokenizer_obj = dictionary.Dictionary().create()
                mode = tokenizer.Tokenizer.SplitMode.C
                tokens = tokenizer_obj.tokenize(content, mode)
                
                # åè©ã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãï¼‰
                nouns = []
                raw_nouns_sample = []  # ãƒ‡ãƒãƒƒã‚°ç”¨ã‚µãƒ³ãƒ—ãƒ«
                
                for i, t in enumerate(tokens):
                    surface = t.surface()
                    if "åè©" in t.part_of_speech() and len(surface) > 1:
                        # ãƒ‡ãƒãƒƒã‚°ç”¨ã‚µãƒ³ãƒ—ãƒ«åé›†ï¼ˆæœ€åˆã®10å€‹ã¾ã§ï¼‰
                        if len(raw_nouns_sample) < 10:
                            raw_nouns_sample.append(surface)
                        
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                        cleaned_surface = clean_keyword(surface)
                        if cleaned_surface and len(cleaned_surface) > 1:  # ç©ºæ–‡å­—ã‚„1æ–‡å­—ã‚’é™¤å¤–
                            nouns.append(cleaned_surface)
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ›
                if raw_nouns_sample:
                    print(f"ğŸ“ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ã‚µãƒ³ãƒ—ãƒ« ({p.name}): {raw_nouns_sample}")
                    print(f"ğŸ“ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã‚µãƒ³ãƒ—ãƒ« ({p.name}): {nouns[:10]}")
                
                # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
                if nouns:
                    word_counts = Counter(nouns)
                    top_keywords = [word for word, count in word_counts.most_common(5)]
                    print(f"ğŸ”‘ æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ({p.name}): {top_keywords}")
                else:
                    top_keywords = []
                    print(f"âš ï¸ åè©ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {p.name}")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¨­å®š
                doc.metadata["top_keywords"] = " / ".join(top_keywords)
                
            except Exception as e:
                print(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({p.name}): {e}")
                import traceback
                print(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
                doc.metadata["top_keywords"] = ""

            # âœï¸ æ–‡å­—æ•°ãƒ»è¡Œæ•°ãªã©ï¼ˆè¿½åŠ ã§å½¹ç«‹ã¤ï¼‰
            doc.metadata["num_chars"] = len(content)
            doc.metadata["num_lines"] = len(content.splitlines())

        docs_all.extend(docs)

def run_company_doc_chain(param):
    """
    ä¼šç¤¾ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    return run_doc_chain_base("company", param)

def run_service_doc_chain(param):
    """
    ã‚µãƒ¼ãƒ“ã‚¹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    return run_doc_chain_base("service", param)

def run_customer_doc_chain(param):
    """
    é¡§å®¢ã¨ã®ã‚„ã‚Šå–ã‚Šã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤
    
    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    return run_doc_chain_base("customer", param)

def run_manual_doc_chain(param):
    """
    æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚„æ‰‹é †æ›¸ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    return run_doc_chain_base("manual", param)

def run_policy_doc_chain(param):
    """
    åˆ©ç”¨è¦ç´„ãƒ»ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ»è¿”å“ãªã©ã®åˆ¶åº¦ãƒ»ãƒãƒªã‚·ãƒ¼ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    return run_doc_chain_base("policy", param)


def run_sustainability_doc_chain(param):
    """
    ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ãƒ»ã‚¨ã‚·ã‚«ãƒ«æ´»å‹•ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    return run_doc_chain_base("sustainability", param)

def delete_old_conversation_log(result):
    """
    å¤ã„ä¼šè©±å±¥æ­´ã®å‰Šé™¤

    Args:
        result: LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
    response_tokens = len(st.session_state.enc.encode(result))
    # éå»ã®ä¼šè©±å±¥æ­´ã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŠ ç®—
    st.session_state.total_tokens += response_tokens

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒä¸Šé™å€¤ã‚’ä¸‹å›ã‚‹ã¾ã§ã€é †ã«å¤ã„ä¼šè©±å±¥æ­´ã‚’å‰Šé™¤
    while st.session_state.total_tokens > ct.MAX_ALLOWED_TOKENS:
        # æœ€ã‚‚å¤ã„ä¼šè©±å±¥æ­´ã‚’å‰Šé™¤
        removed_message = st.session_state.chat_history.pop(1)
        # æœ€ã‚‚å¤ã„ä¼šè©±å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
        removed_tokens = len(st.session_state.enc.encode(removed_message.content))
        # éå»ã®ä¼šè©±å±¥æ­´ã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‹ã‚‰ã€æœ€ã‚‚å¤ã„ä¼šè©±å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¼•ã
        st.session_state.total_tokens -= removed_tokens

def notice_slack(chat_message):
    """
    å•ã„åˆã‚ã›å†…å®¹ã®Slackã¸ã®é€šçŸ¥ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        å•ã„åˆã‚ã›ã‚µãƒ³ã‚¯ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info("ğŸš€ Slacké€šçŸ¥å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")

    try:
        # === é…å»¶åˆæœŸåŒ–ãƒã‚§ãƒƒã‚¯ï¼ˆSlackç”¨ï¼‰ ===
        if "agent_executor" not in st.session_state:
            logger.info("ğŸ”„ Slackå‡¦ç†ã®ãŸã‚é…å»¶åˆæœŸåŒ–ã‚’å®Ÿè¡Œã—ã¾ã™")
            try:
                from initialize import initialize_heavy_components
                with st.spinner("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
                    initialize_heavy_components()
            except Exception as init_error:
                logger.error(f"âŒ é…å»¶åˆæœŸåŒ–ã«å¤±æ•—: {init_error}")
                return "ãŠå•ã„åˆã‚ã›ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç›´æ¥ãŠé›»è©±ã§ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"

        # === Step 1: æ‹…å½“è€…é¸å®š ===
        logger.info("ğŸ‘¥ æ‹…å½“è€…é¸å®šã‚’é–‹å§‹")
        target_employees = select_responsible_employees(chat_message)
        
        # === Step 2: SlackIDå–å¾—ã¨é€šçŸ¥å¯¾è±¡ã®æ±ºå®š ===
        if target_employees:
            # é©åˆ‡ãªæ‹…å½“è€…ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            slack_ids = get_slack_ids(target_employees)
            slack_id_text = create_slack_id_text(slack_ids)
            logger.info(f"ğŸ“§ é€šçŸ¥å¯¾è±¡SlackID: {slack_id_text}")
            notification_type = "specific_users"
        else:
            # é©åˆ‡ãªæ‹…å½“è€…ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯@channelã§å…¨å“¡ã«é€šçŸ¥
            logger.warning("âš ï¸ é©åˆ‡ãªæ‹…å½“è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚@channelã§å…¨å“¡ã«é€šçŸ¥ã—ã¾ã™")
            slack_id_text = "@channel"
            notification_type = "channel_all"

        # === Step 3: å‚è€ƒæƒ…å ±å–å¾— ===
        logger.info("ğŸ“š å‚è€ƒæƒ…å ±ã‚’å–å¾—ä¸­")
        knowledge_context = get_knowledge_context_for_slack(chat_message)

        # === Step 4: ç¾åœ¨æ—¥æ™‚å–å¾— ===
        now_datetime = get_datetime()
        user_email = st.session_state.get("user_email", "æœªå…¥åŠ›")

        # === Step 5: Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ ===
        logger.info("âœï¸ Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆä¸­")
        slack_message = generate_slack_message_with_fallback(
            slack_id_text, chat_message, knowledge_context, 
            now_datetime, user_email, notification_type
        )

        # === Step 6: Slacké€ä¿¡ ===
        logger.info("ğŸ“¤ Slackã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ä¸­")
        success = send_to_slack_channel(slack_message, "customer-contact2")
        
        if success:
            logger.info("âœ… Slacké€šçŸ¥ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            return ct.CONTACT_THANKS_MESSAGE
        else:
            logger.error("âŒ Slacké€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return "ãŠå•ã„åˆã‚ã›ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç›´æ¥ãŠé›»è©±ã§ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"

    except Exception as e:
        logger.error(f"âŒ Slacké€šçŸ¥å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return "ãŠå•ã„åˆã‚ã›ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç›´æ¥ãŠé›»è©±ã§ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"


def select_responsible_employees(chat_message):
    """
    å•ã„åˆã‚ã›å†…å®¹ã«åŸºã¥ã„ã¦æ‹…å½“è€…ã‚’é¸å®š

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        é¸å®šã•ã‚ŒãŸæ‹…å½“è€…ãƒªã‚¹ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    try:
        # å¾“æ¥­å“¡æƒ…å ±ã¨å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
        loader = CSVLoader(ct.EMPLOYEE_FILE_PATH, encoding=ct.CSV_ENCODING)
        docs = loader.load()
        loader = CSVLoader(ct.INQUIRY_HISTORY_FILE_PATH, encoding=ct.CSV_ENCODING)
        docs_history = loader.load()

        # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        for doc in docs:
            doc.page_content = adjust_string(doc.page_content)
            for key in doc.metadata:
                doc.metadata[key] = adjust_string(doc.metadata[key])

        for doc in docs_history:
            doc.page_content = adjust_string(doc.page_content)
            for key in doc.metadata:
                doc.metadata[key] = adjust_string(doc.metadata[key])

        # å‚ç…§ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢
        docs_all = adjust_reference_data(docs, docs_history)
        
        # Retrieverã®ä½œæˆï¼ˆFaissç‰ˆï¼‰
        docs_all_page_contents = [doc.page_content for doc in docs_all]
        embeddings = OpenAIEmbeddings()
        db = FAISS.from_documents(docs_all, embeddings)
        retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})
        
        bm25_retriever = BM25Retriever.from_texts(
            docs_all_page_contents,
            preprocess_func=preprocess_func,
            k=ct.TOP_K
        )
        
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, retriever],
            weights=ct.RETRIEVER_WEIGHTS
        )

        # é–¢é€£æ€§ã®é«˜ã„å¾“æ¥­å“¡æƒ…å ±ã‚’å–å¾—
        employees = ensemble_retriever.invoke(chat_message)
        context = get_context(employees)

        # æ‹…å½“è€…IDé¸å®šã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", ct.SYSTEM_PROMPT_EMPLOYEE_SELECTION)
        ])
        
        output_parser = CommaSeparatedListOutputParser()
        format_instruction = output_parser.get_format_instructions()

        messages = prompt_template.format_prompt(
            employee_context=context, 
            query=chat_message, 
            format_instruction=format_instruction
        ).to_messages()
        
        employee_id_response = st.session_state.llm(messages)
        employee_ids = output_parser.parse(employee_id_response.content)

        # é¸å®šã•ã‚ŒãŸæ‹…å½“è€…æƒ…å ±ã‚’å–å¾—
        target_employees = get_target_employees(employees, employee_ids)
        
        logger.info(f"ğŸ‘¥ é¸å®šã•ã‚ŒãŸæ‹…å½“è€…æ•°: {len(target_employees)}")
        return target_employees

    except Exception as e:
        logger.error(f"âŒ æ‹…å½“è€…é¸å®šã§ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def generate_slack_message_with_fallback(slack_id_text, query, knowledge_context, now_datetime, user_email, notification_type):
    """
    Slackç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆï¼ˆ@channelå¯¾å¿œç‰ˆï¼‰

    Args:
        slack_id_text: ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å¯¾è±¡ã®SlackIDæ–‡å­—åˆ— ã¾ãŸã¯ "@channel"
        query: å•ã„åˆã‚ã›å†…å®¹
        knowledge_context: å‚è€ƒæƒ…å ±
        now_datetime: ç¾åœ¨æ—¥æ™‚
        user_email: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
        notification_type: "specific_users" ã¾ãŸã¯ "channel_all"

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸSlackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    try:
        # é€šçŸ¥ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´
        if notification_type == "channel_all":
            # @channelã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            template = ct.SYSTEM_PROMPT_NOTICE_SLACK_CHANNEL
            
            prompt = PromptTemplate(
                input_variables=["query", "knowledge_context", "now_datetime", "user_email"],
                template=template,
            )
            
            prompt_message = prompt.format(
                query=query,
                knowledge_context=knowledge_context,
                now_datetime=now_datetime,
                user_email=user_email,
                GOOGLE_SHEET_URL=ct.GOOGLE_SHEET_URL,
                WEB_URL=ct.WEB_URL
            )
        else:
            # ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = PromptTemplate(
                input_variables=["slack_id_text", "query", "knowledge_context", "now_datetime", "user_email"],
                template=ct.SYSTEM_PROMPT_NOTICE_SLACK,
            )
            
            prompt_message = prompt.format(
                slack_id_text=slack_id_text,
                query=query,
                knowledge_context=knowledge_context,
                now_datetime=now_datetime,
                user_email=user_email,
                GOOGLE_SHEET_URL=ct.GOOGLE_SHEET_URL,
                WEB_URL=ct.WEB_URL
            )

        # LLMã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
        response = st.session_state.llm.invoke([{"role": "user", "content": prompt_message}])
        generated_message = response.content if hasattr(response, 'content') else str(response)
        
        # @channelã®å ´åˆã¯å…ˆé ­ã«è¿½åŠ 
        if notification_type == "channel_all":
            generated_message = f"@channel\n\n{generated_message}"
        
        logger.info("âœ… Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆå®Œäº†")
        return generated_message

    except Exception as e:
        logger.error(f"âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if notification_type == "channel_all":
            fallback_message = f"""
@channel

ã€ç·Šæ€¥ã€‘é©åˆ‡ãªæ‹…å½“è€…ãŒç‰¹å®šã§ããªã„æ–°ã—ã„ãŠå•ã„åˆã‚ã›ãŒå±Šãã¾ã—ãŸã€‚

ã€å•ã„åˆã‚ã›å†…å®¹ã€‘
{query}

ã€å•ã„åˆã‚ã›è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€‘
{user_email}

ã€æ—¥æ™‚ã€‘
{now_datetime}

ã©ãªãŸã‹å¯¾å¿œå¯èƒ½ãªæ–¹ã¯ã€ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
            """.strip()
        else:
            fallback_message = f"""
æ–°ã—ã„ãŠå•ã„åˆã‚ã›ãŒå±Šãã¾ã—ãŸã€‚

ã€å•ã„åˆã‚ã›å†…å®¹ã€‘
{query}

ã€å•ã„åˆã‚ã›è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€‘
{user_email}

ã€æ—¥æ™‚ã€‘
{now_datetime}

æ‹…å½“è€…ã®çš†æ§˜ã€å¯¾å¿œã‚’ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚
            """.strip()
        
        return fallback_message


def send_to_slack_channel(message, channel_name):
    """
    Slackãƒãƒ£ãƒ³ãƒãƒ«ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡

    Args:
        message: é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        channel_name: é€ä¿¡å…ˆãƒãƒ£ãƒ³ãƒãƒ«å

    Returns:
        bool: é€ä¿¡æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    try:
        # Slack Bot Tokenã‚’å–å¾—
        bot_token = safe_get_secret("SLACK_BOT_TOKEN")
        if not bot_token:
            logger.error("âŒ SLACK_BOT_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False

        # Slack WebClientåˆæœŸåŒ–
        client = WebClient(token=bot_token)
        
        # ãƒãƒ£ãƒ³ãƒãƒ«ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
        response = client.chat_postMessage(
            channel=f"#{channel_name}",
            text=message,
            username="å•ã„åˆã‚ã›ãƒœãƒƒãƒˆ",
            icon_emoji=":robot_face:"
        )
        
        if response["ok"]:
            logger.info(f"âœ… Slacké€ä¿¡æˆåŠŸ: ãƒãƒ£ãƒ³ãƒãƒ« #{channel_name}")
            return True
        else:
            logger.error(f"âŒ Slacké€ä¿¡å¤±æ•—: {response.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            return False

    except SlackApiError as e:
        logger.error(f"âŒ Slack API ã‚¨ãƒ©ãƒ¼: {e.response['error']}")
        return False
    except Exception as e:
        logger.error(f"âŒ Slacké€ä¿¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def get_knowledge_context_for_slack(chat_message):
    """
    Slacké€šçŸ¥ç”¨ã®å‚è€ƒæƒ…å ±ã‚’å–å¾—ï¼ˆGoogle Sheets + Webæ¤œç´¢ï¼‰

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        å‚è€ƒæƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    knowledge_context = ""

    try:
        # === Google Sheets ã‹ã‚‰Q&Aå–å¾— ===
        logger.info("ğŸ“Š Google Sheetsã‹ã‚‰æƒ…å ±å–å¾—ä¸­")
        try:
            scope = [
                "https://spreadsheets.google.com/feeds",
                "https://www.googleapis.com/auth/drive"
            ]
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                'secrets/service_account.json', scope
            )
            client = gspread.authorize(creds)
            sheet = client.open_by_url(ct.GOOGLE_SHEET_URL).sheet1
            rows = sheet.get_all_records()

            sheets_context = "ã€Google Sheetsã‹ã‚‰å–å¾—ã—ãŸç¤¾å†…Q&Aã€‘\n"
            for i, row in enumerate(rows[:10], 1):  # æœ€åˆã®10ä»¶ã¾ã§
                q = row.get("è³ªå•", "")
                a = row.get("å›ç­”", "")
                source = row.get("æ ¹æ‹ è³‡æ–™", "")
                if q and a:
                    sheets_context += f"{i}. Q: {q}\n   A: {a}\n"
                    if source:
                        sheets_context += f"   æ ¹æ‹ : {source}\n"
                    sheets_context += "\n"

            knowledge_context += sheets_context + "\n" + "="*50 + "\n"
            logger.info(f"âœ… Google Sheetsæƒ…å ±å–å¾—å®Œäº†: {len(rows)}ä»¶")

        except Exception as e:
            logger.warning(f"âš ï¸ Google Sheetså–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            knowledge_context += "ã€Google Sheetsæƒ…å ±ã€‘å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n\n"

        # === Webæ¤œç´¢ï¼ˆpip-maker.comï¼‰===
        logger.info("ğŸŒ Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­")
        try:
            search_wrapper = GoogleSearchAPIWrapper()
            search_query = f"site:pip-maker.com {chat_message}"
            web_results = search_wrapper.run(search_query)
            
            web_context = "ã€pip-maker.comã‹ã‚‰ã®æ¤œç´¢çµæœã€‘\n"
            web_context += web_results[:1000] + "...\n\n"  # æœ€åˆã®1000æ–‡å­—ã¾ã§
            
            knowledge_context += web_context
            logger.info("âœ… Webæ¤œç´¢å®Œäº†")

        except Exception as e:
            logger.warning(f"âš ï¸ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            knowledge_context += "ã€Webæ¤œç´¢æƒ…å ±ã€‘å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n\n"

        return knowledge_context

    except Exception as e:
        logger.error(f"âŒ å‚è€ƒæƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return "å‚è€ƒæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"


def adjust_reference_data(docs, docs_history):
    """
    Slacké€šçŸ¥ç”¨ã®å‚ç…§å…ˆãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢

    Args:
        docs: å¾“æ¥­å“¡æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿
        docs_history: å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿

    Returns:
        å¾“æ¥­å“¡æƒ…å ±ã¨å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã®çµåˆãƒ†ã‚­ã‚¹ãƒˆ
    """

    docs_all = []
    for row in docs:
        # å¾“æ¥­å“¡IDã®å–å¾—
        row_lines = row.page_content.split("\n")
        row_dict = {item.split(": ")[0]: item.split(": ")[1] for item in row_lines}
        employee_id = row_dict["å¾“æ¥­å“¡ID"]

        doc = ""

        # å–å¾—ã—ãŸå¾“æ¥­å“¡IDã«ç´ã¥ãå•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã‚’å–å¾—
        same_employee_inquiries = []
        for row_history in docs_history:
            row_history_lines = row_history.page_content.split("\n")
            row_history_dict = {item.split(": ")[0]: item.split(": ")[1] for item in row_history_lines}
            if row_history_dict["å¾“æ¥­å“¡ID"] == employee_id:
                same_employee_inquiries.append(row_history_dict)

        if same_employee_inquiries:
            doc += "ã€å¾“æ¥­å“¡æƒ…å ±ã€‘\n"
            row_data = "\n".join(row_lines)
            doc += row_data + "\n=================================\n"
            doc += "ã€ã“ã®å¾“æ¥­å“¡ã®å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã€‘\n"
            for inquiry_dict in same_employee_inquiries:
                for key, value in inquiry_dict.items():
                    doc += f"{key}: {value}\n"
                doc += "---------------\n"

            new_doc = Document(page_content=doc, metadata={})
        else:
            new_doc = Document(page_content=row.page_content, metadata={})

        docs_all.append(new_doc)
    
    return docs_all

def get_target_employees(employees, employee_ids):
    """
    å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§ã®å–å¾—

    Args:
        employees: å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§
        employee_ids: å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒã€Œç‰¹ã«ã€é«˜ã„å¾“æ¥­å“¡ã®IDä¸€è¦§

    Returns:
        å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒã€Œç‰¹ã«ã€é«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§
    """

    target_employees = []
    duplicate_check = []
    target_text = "å¾“æ¥­å“¡ID"
    for employee in employees:
        # å¾“æ¥­å“¡IDã®å–å¾—
        num = employee.page_content.find(target_text)
        employee_id = employee.page_content[num+len(target_text)+2:].split("\n")[0]
        # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ã‚’ã€IDã§ç…§åˆã—ã¦å–å¾—ï¼ˆé‡è¤‡é™¤å»ï¼‰
        if employee_id in employee_ids:
            if employee_id in duplicate_check:
                continue
            duplicate_check.append(employee_id)
            target_employees.append(employee)
    
    return target_employees

def get_slack_ids(target_employees):
    """
    SlackIDã®ä¸€è¦§ã‚’å–å¾—

    Args:
        target_employees: å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§

    Returns:
        SlackIDã®ä¸€è¦§
    """

    target_text = "SlackID"
    slack_ids = []
    for employee in target_employees:
        num = employee.page_content.find(target_text)
        slack_id = employee.page_content[num+len(target_text)+2:].split("\n")[0]
        slack_ids.append(slack_id)
    
    return slack_ids

def create_slack_id_text(slack_ids):
    """
    SlackIDã®ä¸€è¦§ã‚’å–å¾—

    Args:
        slack_ids: SlackIDã®ä¸€è¦§

    Returns:
        SlackIDã‚’ã€Œã¨ã€ã§ç¹‹ã„ã ãƒ†ã‚­ã‚¹ãƒˆ
    """
    slack_id_text = ""
    for i, id in enumerate(slack_ids):
        slack_id_text += f"ã€Œ{id}ã€"
        # æœ€å¾Œã®SlackIDä»¥å¤–ã€é€£çµå¾Œã«ã€Œã¨ã€ã‚’è¿½åŠ 
        if not i == len(slack_ids)-1:
            slack_id_text += "ã¨"
    
    return slack_id_text

def get_context(docs):
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€ãŸã‚ã®å¾“æ¥­å“¡æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆ
    Args:
        docs: å¾“æ¥­å“¡æƒ…å ±ã®ä¸€è¦§

    Returns:
        ç”Ÿæˆã—ãŸå¾“æ¥­å“¡æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆ
    """

    context = ""
    for i, doc in enumerate(docs, start=1):
        context += "===========================================================\n"
        context += f"{i}äººç›®ã®å¾“æ¥­å“¡æƒ…å ±\n"
        context += "===========================================================\n"
        context += doc.page_content + "\n\n"

    return context

def get_datetime():
    """
    ç¾åœ¨æ—¥æ™‚ã‚’å–å¾—

    Returns:
        ç¾åœ¨æ—¥æ™‚
    """

    dt_now = datetime.datetime.now()
    now_datetime = dt_now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')

    return now_datetime

def preprocess_func(text):
    """
    å½¢æ…‹ç´ è§£æã«ã‚ˆã‚‹æ—¥æœ¬èªã®å˜èªåˆ†å‰²
    Args:
        text: å˜èªåˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        å˜èªåˆ†å‰²ã‚’å®Ÿæ–½å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
    """

    tokenizer_obj = dictionary.Dictionary(dict="full").create()
    mode = tokenizer.Tokenizer.SplitMode.A
    tokens = tokenizer_obj.tokenize(text ,mode)
    words = [token.surface() for token in tokens]
    words = list(set(words))

    return words

def adjust_string(s):
    """
    Windowsç’°å¢ƒã§RAGãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã‚ˆã†èª¿æ•´
    
    Args:
        s: èª¿æ•´ã‚’è¡Œã†æ–‡å­—åˆ—
    
    Returns:
        èª¿æ•´ã‚’è¡Œã£ãŸæ–‡å­—åˆ—
    """
    # èª¿æ•´å¯¾è±¡ã¯æ–‡å­—åˆ—ã®ã¿
    if type(s) is not str:
        return s

    # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
    if sys.platform.startswith("win"):
        s = unicodedata.normalize('NFC', s)
        s = s.encode("cp932", "ignore").decode("cp932")
        return s
    
    # OSãŒWindowsä»¥å¤–ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    return s

def filter_chunks_by_top_keywords(docs, query):
    """
    top_keywords ã‚’ä½¿ã£ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    Args:
        docs: æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆï¼ˆDocumentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›

    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆï¼ˆæ¡ä»¶ã«ä¸€è‡´ã—ãªã„å ´åˆã¯å…ƒã®docsã‚’è¿”ã™ï¼‰
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    try:
        # ã‚¯ã‚¨ãƒªã‹ã‚‰åè©ã‚’æŠ½å‡º
        tokenizer_obj = dictionary.Dictionary().create()
        mode = tokenizer.Tokenizer.SplitMode.C
        tokens = tokenizer_obj.tokenize(query, mode)
        query_nouns = set([
            t.surface() 
            for t in tokens
            if "åè©" in t.part_of_speech() and len(t.surface()) > 1
        ])
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾è±¡ã‚¯ã‚¨ãƒªåè©: {query_nouns}")
        logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‰ãƒãƒ£ãƒ³ã‚¯ä»¶æ•°: {len(docs)}")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        filtered_docs = []
        for doc in docs:
            top_keywords_str = doc.metadata.get("top_keywords", "")
            if top_keywords_str:
                # top_keywordsãŒ " / " ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã®å‡¦ç†
                top_keywords = [kw.strip() for kw in top_keywords_str.split(" / ") if kw.strip()]
                
                # ã‚¯ã‚¨ãƒªã®åè©ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                if any(kw in query_nouns for kw in top_keywords if kw):
                    filtered_docs.append(doc)
                    logger.info(f"ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {[kw for kw in top_keywords if kw in query_nouns]}")
        
        logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œãƒãƒ£ãƒ³ã‚¯ä»¶æ•°: {len(filtered_docs)}")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç©ºã®å ´åˆã¯å…ƒã®docsã‚’è¿”ã™ï¼ˆfallbackï¼‰
        if not filtered_docs:
            logger.info("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç©ºã®ãŸã‚ã€å…ƒã®docsã‚’è¿”ã—ã¾ã™")
            return docs
        
        return filtered_docs
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return docs  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®docsã‚’è¿”ã™

def execute_agent_or_chain(chat_message):
    """
    AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚‚ã—ãã¯AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãªã—ã®RAGã®Chainã‚’å®Ÿè¡Œï¼ˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    logger = logging.getLogger(ct.LOGGER_NAME)

    # === é…å»¶åˆæœŸåŒ–ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰ ===
    if "agent_executor" not in st.session_state:
        logger.info("ğŸ”„ agent_executorãŒæœªåˆæœŸåŒ–ã®ãŸã‚ã€é…å»¶åˆæœŸåŒ–ã‚’å®Ÿè¡Œã—ã¾ã™")
        
        init_placeholder = st.empty()
        with init_placeholder:
            st.info("ğŸ”„ åˆå›ã‚¢ã‚¯ã‚»ã‚¹ã®ãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™... ï¼ˆ30-60ç§’ç¨‹åº¦ãŠå¾…ã¡ãã ã•ã„ï¼‰")
        
        try:
            from initialize import initialize_heavy_components
            initialize_heavy_components()
            logger.info("âœ… é…å»¶åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            with init_placeholder:
                st.success("âœ… åˆæœŸåŒ–å®Œäº†ï¼å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
                
        except Exception as init_error:
            logger.error(f"âŒ é…å»¶åˆæœŸåŒ–ã«å¤±æ•—: {init_error}")
            with init_placeholder:
                st.error("âŒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚"
        finally:
            import time
            time.sleep(1)
            init_placeholder.empty()

    # === å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®è¨˜éŒ² ===
    logger.info(f"ğŸ¯ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {st.session_state.agent_mode}")
    logger.info(f"ğŸ“ å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {chat_message}")

    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆ
    if st.session_state.agent_mode == ct.AI_AGENT_MODE_ON:
        logger.info("ğŸ¤– AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
        
        try:
            # === ä¿®æ­£: StreamlitCallbackHandlerã®å®‰å…¨ãªä½¿ç”¨ ===
            # ã‚³ãƒ³ãƒ†ãƒŠã‚’äº‹å‰ã«ä½œæˆã—ã¦ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å ´æ‰€ã‚’ç¢ºä¿
            callback_container = st.container()
            
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã§CallbackHandlerã‚’ä½œæˆ
            try:
                st_callback = StreamlitCallbackHandler(
                    parent_container=callback_container,
                    max_thought_containers=4,  # æ€è€ƒéç¨‹ã®è¡¨ç¤ºæ•°ã‚’åˆ¶é™
                    expand_new_thoughts=True,
                    collapse_completed_thoughts=True
                )
                
                # Agentå®Ÿè¡Œ
                result = st.session_state.agent_executor.invoke(
                    {"input": chat_message}, 
                    {"callbacks": [st_callback]}
                )
                response = result["output"]
                
            except Exception as callback_error:
                logger.warning(f"âš ï¸ StreamlitCallbackHandlerã§ã‚¨ãƒ©ãƒ¼: {callback_error}")
                logger.info("ğŸ”„ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ã§Agentå®Ÿè¡Œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                
                # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ã§å†å®Ÿè¡Œ
                result = st.session_state.agent_executor.invoke(
                    {"input": chat_message},
                    {"callbacks": []}  # ç©ºã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
                )
                response = result["output"]
                
                # æ‰‹å‹•ã§æ€è€ƒéç¨‹ã‚’è¡¨ç¤º
                with callback_container:
                    st.info("ğŸ¤– AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                
        except Exception as agent_error:
            logger.error(f"âŒ Agentå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼: {agent_error}")
            response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
            
    else:
        # === é€šå¸¸RAGãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰ ===
        logger.info("ğŸ” é€šå¸¸RAGãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ - æŸ”è»Ÿã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°é©ç”¨")
        
        try:
            retriever = create_retriever(ct.DB_ALL_PATH)
            original_docs = retriever.get_relevant_documents(chat_message)
            logger.info(f"ğŸ“š é€šå¸¸æ¤œç´¢çµæœ: {len(original_docs)}ä»¶")

            logger.info("ğŸ§  æŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚’é–‹å§‹")
            filtered_docs = filter_chunks_by_flexible_keywords(original_docs, chat_message)
            logger.info(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œ: {len(filtered_docs)}ä»¶")

            if filtered_docs:
                logger.info("ğŸ“– ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œæ–‡æ›¸ã§RAGå®Ÿè¡Œ")
                context = "\n\n".join([doc.page_content for doc in filtered_docs[:ct.TOP_K]])
                
                question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
                messages = [
                    {"role": "system", "content": question_answer_template.format(context=context)},
                    {"role": "user", "content": chat_message}
                ]
                
                response_obj = st.session_state.llm.invoke(messages)
                response = response_obj.content if hasattr(response_obj, 'content') else str(response_obj)
                logger.info("âœ… ã‚«ã‚¹ã‚¿ãƒ RAGå‡¦ç†å®Œäº†")
            else:
                logger.info("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç©º - é€šå¸¸RAGãƒã‚§ãƒ¼ãƒ³ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                if "rag_chain" not in st.session_state:
                    logger.warning("âš ï¸ rag_chainã‚‚æœªåˆæœŸåŒ–ã§ã™")
                    return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ãŒå®Œå…¨ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚"
                
                result = st.session_state.rag_chain.invoke({
                    "input": chat_message,
                    "chat_history": st.session_state.chat_history
                })
                response = result["answer"]

            st.session_state.chat_history.extend([
                HumanMessage(content=chat_message),
                AIMessage(content=response)
            ])
            
        except Exception as e:
            logger.error(f"âŒ RAGå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            
            try:
                if "rag_chain" in st.session_state:
                    result = st.session_state.rag_chain.invoke({
                        "input": chat_message,
                        "chat_history": st.session_state.chat_history
                    })
                    response = result["answer"]
                    logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œäº†")
                else:
                    logger.error("âŒ rag_chainã‚‚å­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ä¸å¯èƒ½")
                    response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã«å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚"
            except Exception as e2:
                logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")
                response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã«å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚"

    # ãƒ•ãƒ©ã‚°è¨­å®š
    if response != ct.NO_DOC_MATCH_MESSAGE:
        st.session_state.answer_flg = True

    logger.info(f"ğŸ“¤ æœ€çµ‚å›ç­”: {response[:100]}...")
    return response

def test_keyword_filter():
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆé–¢æ•°
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    test_queries = [
        "SNSæŠ•ç¨¿ã«é–¢ã™ã‚‹ç‰¹å…¸ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "æµ·å¤–é…é€ã¯å¯¾å¿œã—ã¦ã„ã¾ã™ã‹ï¼Ÿ", 
        "åœ°åŸŸè²¢çŒ®æ´»å‹•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "å—è³æ­´ã‚’æ•™ãˆã¦ãã ã•ã„",
        "æ ªä¸»å„ªå¾…åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦",
        "ç’°å¢ƒã¸ã®å–ã‚Šçµ„ã¿ã‚’çŸ¥ã‚ŠãŸã„",
        "ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã®æ–™é‡‘ã¯ï¼Ÿ"
    ]
    
    retriever = create_retriever(ct.DB_ALL_PATH)
    
    for query in test_queries:
        print(f"\n{'='*100}")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª: {query}")
        print('='*100)
        
        # é€šå¸¸æ¤œç´¢
        original_docs = retriever.get_relevant_documents(query)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        filtered_docs = filter_chunks_by_top_keywords(original_docs, query)
        
        print(f"ğŸ“Š çµæœ:")
        print(f"   - é€šå¸¸æ¤œç´¢: {len(original_docs)}ä»¶")
        print(f"   - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œ: {len(filtered_docs)}ä»¶")
        print(f"   - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åŠ¹æœ: {(1-len(filtered_docs)/len(original_docs))*100:.1f}%å‰Šæ¸›")

def test_flexible_keyword_filter():
    """
    æŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆé–¢æ•°
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    # å•é¡Œã®ã‚ã£ãŸã‚¯ã‚¨ãƒªã‚’å«ã‚€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_queries = [
        "å—è³æ­´ã‚’æ•™ãˆã¦ãã ã•ã„",  # ãƒ¡ã‚¤ãƒ³ã®å•é¡Œã‚¯ã‚¨ãƒª
        "SNSæŠ•ç¨¿ã«é–¢ã™ã‚‹ç‰¹å…¸ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "æµ·å¤–é…é€ã¯å¯¾å¿œã—ã¦ã„ã¾ã™ã‹ï¼Ÿ", 
        "åœ°åŸŸè²¢çŒ®æ´»å‹•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "æ ªä¸»å„ªå¾…åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦",
        "ç’°å¢ƒã¸ã®å–ã‚Šçµ„ã¿ã‚’çŸ¥ã‚ŠãŸã„",
        "ä¼šç¤¾ã®å®Ÿç¸¾ã‚’æ•™ãˆã¦",
        "ã‚¢ãƒ¯ãƒ¼ãƒ‰ã‚’å—è³ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ"
    ]
    
    retriever = create_retriever(ct.DB_ALL_PATH)
    
    for query in test_queries:
        print(f"\n{'='*100}")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª: {query}")
        print('='*100)
        
        # é€šå¸¸æ¤œç´¢
        original_docs = retriever.get_relevant_documents(query)
        
        # å¾“æ¥ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        old_filtered_docs = filter_chunks_by_top_keywords(original_docs, query)
        
        # æ–°ã—ã„æŸ”è»Ÿãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        new_filtered_docs = filter_chunks_by_flexible_keywords(original_docs, query)
        
        print(f"ğŸ“Š çµæœæ¯”è¼ƒ:")
        print(f"   - é€šå¸¸æ¤œç´¢: {len(original_docs)}ä»¶")
        print(f"   - å¾“æ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {len(old_filtered_docs)}ä»¶")
        print(f"   - æŸ”è»Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {len(new_filtered_docs)}ä»¶")
        print(f"   - æ”¹å–„åŠ¹æœ: {len(new_filtered_docs) - len(old_filtered_docs):+d}ä»¶")
        
@st.cache_resource
def create_cached_retriever(db_path):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãã®Retrieverä½œæˆ
    
    Args:
        db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        
    Returns:
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸRetrieverã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ—ƒï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãRetrieverä½œæˆ: {db_path}")
    
    return create_retriever(db_path)

@st.cache_resource
def create_cached_rag_chain(db_path):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãã®RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆ
    
    Args:
        db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        
    Returns:
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸRAGãƒã‚§ãƒ¼ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ğŸ”— ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãRAGãƒã‚§ãƒ¼ãƒ³ä½œæˆ: {db_path}")
    
    return create_rag_chain(db_path)

def run_lightweight_debug():
    """
    è»½é‡åŒ–ã•ã‚ŒãŸãƒ‡ãƒãƒƒã‚°å‡¦ç†ï¼ˆèµ·å‹•æ™‚é–“çŸ­ç¸®ç”¨ï¼‰
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info("ğŸ”§ è»½é‡ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œä¸­...")
    
    try:
        # æœ€å°é™ã®ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
        test_query = "å—è³æ­´ã‚’æ•™ãˆã¦ãã ã•ã„"
        
        # å½¢æ…‹ç´ è§£æã®ãƒ†ã‚¹ãƒˆ
        from sudachipy import tokenizer, dictionary
        tokenizer_obj = dictionary.Dictionary().create()
        mode = tokenizer.Tokenizer.SplitMode.C
        tokens = tokenizer_obj.tokenize(test_query, mode)
        nouns = [t.surface() for t in tokens if "åè©" in t.part_of_speech()]
        
        logger.info(f"ğŸ§ª è»½é‡ãƒ‡ãƒãƒƒã‚°å®Œäº†: æŠ½å‡ºåè© {nouns}")
        
        # ãƒ•ãƒ©ã‚°è¨­å®š
        st.session_state.retriever_debug_done = True
        st.session_state.flexible_keyword_debug_done = True
        
    except Exception as e:
        logger.warning(f"âš ï¸ è»½é‡ãƒ‡ãƒãƒƒã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")