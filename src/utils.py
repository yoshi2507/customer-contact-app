"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from dotenv import load_dotenv
import streamlit as st
import logging
import sys
import unicodedata
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.schema import HumanMessage, AIMessage
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from typing import List
from sudachipy import tokenizer, dictionary
from langchain_community.agent_toolkits import SlackToolkit
from langchain.agents import AgentType, initialize_agent
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from docx import Document
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain import LLMChain
import datetime
import constants as ct


############################################################
# è¨­å®šé–¢é€£
############################################################
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def create_rag_chain(db_name):
    """
    å¼•æ•°ã¨ã—ã¦æ¸¡ã•ã‚ŒãŸDBå†…ã‚’å‚ç…§ã™ã‚‹RAGã®Chainã‚’ä½œæˆ

    Args:
        db_name: RAGåŒ–å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å
    """
    logger = logging.getLogger(ct.LOGGER_NAME)

    docs_all = []
    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä½¿ã‚ãªã„å ´åˆã®å‡¦ç†
    if db_name == ct.DB_ALL_PATH:
        folders = os.listdir(ct.RAG_TOP_FOLDER_PATH)
        # ã€Œdataã€ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã®å„ãƒ•ã‚©ãƒ«ãƒ€åã«å¯¾ã—ã¦å‡¦ç†
        for folder_path in folders:
            if folder_path.startswith("."):
                continue
            # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
            add_docs(f"{ct.RAG_TOP_FOLDER_PATH}/{folder_path}", docs_all)
    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä½¿ã†å ´åˆã®å‡¦ç†
    else:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åã«å¯¾å¿œã—ãŸã€RAGåŒ–å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ç¾¤ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—
        folder_path = ct.DB_NAMES[db_name]
        # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        add_docs(folder_path, docs_all)

    # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
    for doc in docs_all:
        doc.page_content = adjust_string(doc.page_content)
        for key in doc.metadata:
            doc.metadata[key] = adjust_string(doc.metadata[key])
    
    text_splitter = CharacterTextSplitter(
        chunk_size=ct.CHUNK_SIZE,
        chunk_overlap=ct.CHUNK_OVERLAP,
        separator="\n",
    )
    splitted_docs = text_splitter.split_documents(docs_all)

    embeddings = OpenAIEmbeddings()

    # ã™ã§ã«å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒä½œæˆæ¸ˆã¿ã®å ´åˆã¯èª­ã¿è¾¼ã¿ã€æœªä½œæˆã®å ´åˆã¯æ–°è¦ä½œæˆã™ã‚‹
    if os.path.isdir(db_name):
        db = Chroma(persist_directory=".db", embedding_function=embeddings)
    else:
        db = Chroma.from_documents(splitted_docs, embedding=embeddings, persist_directory=".db")
    retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})

    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        st.session_state.llm, retriever, question_generator_prompt
    )

    question_answer_chain = create_stuff_documents_chain(st.session_state.llm, question_answer_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain



def add_docs(folder_path, docs_all):
    """
    ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—

    Args:
        folder_path: ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        docs_all: å„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    """
    print(f"ğŸ“‚ èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€: {folder_path}")
    print(f"ğŸ“‚ ãƒ•ãƒ«ãƒ‘ã‚¹: {os.path.abspath(folder_path)}")

    files = os.listdir(folder_path)
    for file in files:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’å–å¾—
        file_extension = os.path.splitext(file)[1]
        # æƒ³å®šã—ã¦ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®å ´åˆã®ã¿èª­ã¿è¾¼ã‚€
        if file_extension in ct.SUPPORTED_EXTENSIONS:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã«åˆã£ãŸdata loaderã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            loader = ct.SUPPORTED_EXTENSIONS[file_extension](f"{folder_path}/{file}")
        else:
            continue
        docs = loader.load()
        docs_all.extend(docs)


def run_company_doc_chain(param):
    """
    ä¼šç¤¾ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # ä¼šç¤¾ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸChainã‚’å®Ÿè¡Œã—ã¦LLMã‹ã‚‰ã®å›ç­”å–å¾—
    ai_msg = st.session_state.company_doc_chain.invoke({"input": param, "chat_history": st.session_state.chat_history})
    # ä¼šè©±å±¥æ­´ã¸ã®è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])

    return ai_msg["answer"]

def run_service_doc_chain(param):
    """
    ã‚µãƒ¼ãƒ“ã‚¹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # ã‚µãƒ¼ãƒ“ã‚¹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸChainã‚’å®Ÿè¡Œã—ã¦LLMã‹ã‚‰ã®å›ç­”å–å¾—
    ai_msg = st.session_state.service_doc_chain.invoke({"input": param, "chat_history": st.session_state.chat_history})

    # ä¼šè©±å±¥æ­´ã¸ã®è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])

    return ai_msg["answer"]

def run_customer_doc_chain(param):
    """
    é¡§å®¢ã¨ã®ã‚„ã‚Šå–ã‚Šã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤
    
    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # é¡§å®¢ã¨ã®ã‚„ã‚Šå–ã‚Šã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸChainã‚’å®Ÿè¡Œã—ã¦LLMã‹ã‚‰ã®å›ç­”å–å¾—
    ai_msg = st.session_state.customer_doc_chain.invoke({"input": param, "chat_history": st.session_state.chat_history})

    # ä¼šè©±å±¥æ­´ã¸ã®è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])

    return ai_msg["answer"]

def run_manual_doc_chain(param):
    """
    æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚„æ‰‹é †æ›¸ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # RAG chainã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆ
    ai_msg = st.session_state.manual_doc_chain.invoke({
        "input": param,
        "chat_history": st.session_state.chat_history
    })

    # ä¼šè©±å±¥æ­´ã«è¨˜éŒ²ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    st.session_state.chat_history.extend([
        HumanMessage(content=param),
        AIMessage(content=ai_msg["answer"])
    ])

    return ai_msg["answer"]

def run_policy_doc_chain(param):
    """
    åˆ©ç”¨è¦ç´„ãƒ»ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ»è¿”å“ãªã©ã®åˆ¶åº¦ãƒ»ãƒãƒªã‚·ãƒ¼ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # RAG chainã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆ
    ai_msg = st.session_state.policy_doc_chain.invoke({
        "input": param,
        "chat_history": st.session_state.chat_history
    })

    # ä¼šè©±å±¥æ­´ã«è¨˜éŒ²ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    st.session_state.chat_history.extend([
        HumanMessage(content=param),
        AIMessage(content=ai_msg["answer"])
    ])

    return ai_msg["answer"]


def run_sustainability_doc_chain(param):
    """
    ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ãƒ»ã‚¨ã‚·ã‚«ãƒ«æ´»å‹•ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‚ç…§ã«ç‰¹åŒ–ã—ãŸToolè¨­å®šç”¨ã®é–¢æ•°

    Args:
        param: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # RAG chainã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆ
    ai_msg = st.session_state.sustainability_doc_chain.invoke({
        "input": param,
        "chat_history": st.session_state.chat_history
    })

    # ä¼šè©±å±¥æ­´ã«è¨˜éŒ²ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    st.session_state.chat_history.extend([
        HumanMessage(content=param),
        AIMessage(content=ai_msg["answer"])
    ])

    return ai_msg["answer"]

def delete_old_conversation_log(result):
    """
    å¤ã„ä¼šè©±å±¥æ­´ã®å‰Šé™¤

    Args:
        result: LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
    response_tokens = len(st.session_state.enc.encode(result))
    # éå»ã®ä¼šè©±å±¥æ­´ã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŠ ç®—
    st.session_state.total_tokens += response_tokens

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒä¸Šé™å€¤ã‚’ä¸‹å›ã‚‹ã¾ã§ã€é †ã«å¤ã„ä¼šè©±å±¥æ­´ã‚’å‰Šé™¤
    while st.session_state.total_tokens > ct.MAX_ALLOWED_TOKENS:
        # æœ€ã‚‚å¤ã„ä¼šè©±å±¥æ­´ã‚’å‰Šé™¤
        removed_message = st.session_state.chat_history.pop(1)
        # æœ€ã‚‚å¤ã„ä¼šè©±å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
        removed_tokens = len(st.session_state.enc.encode(removed_message.content))
        # éå»ã®ä¼šè©±å±¥æ­´ã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‹ã‚‰ã€æœ€ã‚‚å¤ã„ä¼šè©±å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¼•ã
        st.session_state.total_tokens -= removed_tokens


def execute_agent_or_chain(chat_message):
    """
    AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚‚ã—ãã¯AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãªã—ã®RAGã®Chainã‚’å®Ÿè¡Œ

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    logger = logging.getLogger(ct.LOGGER_NAME)

    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆ
    if st.session_state.agent_mode == ct.AI_AGENT_MODE_ON:
        # LLMã«ã‚ˆã‚‹å›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
        st_callback = StreamlitCallbackHandler(st.container())
        # Agent Executorã®å®Ÿè¡Œï¼ˆAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä½¿ã†å ´åˆã¯ã€Toolã¨ã—ã¦è¨­å®šã—ãŸé–¢æ•°å†…ã§ä¼šè©±å±¥æ­´ã¸ã®è¿½åŠ å‡¦ç†ã‚’å®Ÿæ–½ï¼‰
        result = st.session_state.agent_executor.invoke({"input": chat_message}, {"callbacks": [st_callback]})
        response = result["output"]
    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆ©ç”¨ã—ãªã„å ´åˆ
    else:
        # RAGã®Chainã‚’å®Ÿè¡Œ
        result = st.session_state.rag_chain.invoke({"input": chat_message, "chat_history": st.session_state.chat_history})
        # ä¼šè©±å±¥æ­´ã¸ã®è¿½åŠ 
        st.session_state.chat_history.extend([HumanMessage(content=chat_message), AIMessage(content=result["answer"])])
        response = result["answer"]

    # LLMã‹ã‚‰å‚ç…§å…ˆã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã—ãŸå›ç­”ãŒè¡Œã‚ã‚ŒãŸå ´åˆã®ã¿ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
    if response != ct.NO_DOC_MATCH_MESSAGE:
        st.session_state.answer_flg = True
    
    return response


def notice_slack(chat_message):
    """
    å•ã„åˆã‚ã›å†…å®¹ã®Slackã¸ã®é€šçŸ¥

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        å•ã„åˆã‚ã›ã‚µãƒ³ã‚¯ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """

    # Slacké€šçŸ¥ç”¨ã®Agent Executorã‚’ä½œæˆ
    toolkit = SlackToolkit()
    tools = toolkit.get_tools()
    agent_executor = initialize_agent(
        llm=st.session_state.llm,
        tools=tools,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION
    )

    # æ‹…å½“è€…å‰²ã‚ŠæŒ¯ã‚Šã«ä½¿ã†ç”¨ã®ã€Œå¾“æ¥­å“¡æƒ…å ±ã€ã¨ã€Œå•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã€ã®èª­ã¿è¾¼ã¿
    loader = CSVLoader(ct.EMPLOYEE_FILE_PATH, encoding=ct.CSV_ENCODING)
    docs = loader.load()
    loader = CSVLoader(ct.INQUIRY_HISTORY_FILE_PATH, encoding=ct.CSV_ENCODING)
    docs_history = loader.load()

    # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
    for doc in docs:
        doc.page_content = adjust_string(doc.page_content)
        for key in doc.metadata:
            doc.metadata[key] = adjust_string(doc.metadata[key])
    for doc in docs_history:
        doc.page_content = adjust_string(doc.page_content)
        for key in doc.metadata:
            doc.metadata[key] = adjust_string(doc.metadata[key])

    # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«ã€å‚ç…§å…ˆãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
    docs_all = adjust_reference_data(docs, docs_history)
    
    # å½¢æ…‹ç´ è§£æã«ã‚ˆã‚‹æ—¥æœ¬èªã®å˜èªåˆ†å‰²ã‚’è¡Œã†ãŸã‚ã€å‚ç…§å…ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
    docs_all_page_contents = []
    for doc in docs_all:
        docs_all_page_contents.append(doc.page_content)

    # Retrieverã®ä½œæˆ
    embeddings = OpenAIEmbeddings()
    db = Chroma.from_documents(docs_all, embedding=embeddings)
    retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})
    bm25_retriever = BM25Retriever.from_texts(
        docs_all_page_contents,
        preprocess_func=preprocess_func,
        k=ct.TOP_K
    )
    retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, retriever],
        weights=ct.RETRIEVER_WEIGHTS
    )

    # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ã®é«˜ã„å¾“æ¥­å“¡æƒ…å ±ã‚’å–å¾—
    employees = retriever.invoke(chat_message)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€ãŸã‚ã®å¾“æ¥­å“¡æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    context = get_context(employees)

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", ct.SYSTEM_PROMPT_EMPLOYEE_SELECTION)
    ])
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
    output_parser = CommaSeparatedListOutputParser()
    format_instruction = output_parser.get_format_instructions()

    # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡ã®IDä¸€è¦§ã‚’å–å¾—
    messages = prompt_template.format_prompt(context=context, query=chat_message, format_instruction=format_instruction).to_messages()
    employee_id_response = st.session_state.llm(messages)
    employee_ids = output_parser.parse(employee_id_response.content)

    # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ã‚’ã€IDã§ç…§åˆã—ã¦å–å¾—
    target_employees = get_target_employees(employees, employee_ids)
    
    # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ã®ä¸­ã‹ã‚‰ã€SlackIDã®ã¿ã‚’æŠ½å‡º
    slack_ids = get_slack_ids(target_employees)
    
    # æŠ½å‡ºã—ãŸSlackIDã®é€£çµãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    slack_id_text = create_slack_id_text(slack_ids)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€ãŸã‚ã®ï¼ˆå•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„ï¼‰å¾“æ¥­å“¡æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    context = get_context(target_employees)

    # ç¾åœ¨æ—¥æ™‚ã‚’å–å¾—
    now_datetime = get_datetime()

    # Slacké€šçŸ¥ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    prompt = PromptTemplate(
        input_variables=["slack_id_text", "query", "context", "now_datetime"],
        template=ct.SYSTEM_PROMPT_NOTICE_SLACK,
    )
    prompt_message = prompt.format(slack_id_text=slack_id_text, query=chat_message, context=context, now_datetime=now_datetime)

    # Slacké€šçŸ¥ã®å®Ÿè¡Œ
    agent_executor.invoke({"input": prompt_message})

    return ct.CONTACT_THANKS_MESSAGE


def adjust_reference_data(docs, docs_history):
    """
    Slacké€šçŸ¥ç”¨ã®å‚ç…§å…ˆãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢

    Args:
        docs: å¾“æ¥­å“¡æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿
        docs_history: å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿

    Returns:
        å¾“æ¥­å“¡æƒ…å ±ã¨å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã®çµåˆãƒ†ã‚­ã‚¹ãƒˆ
    """

    docs_all = []
    for row in docs:
        # å¾“æ¥­å“¡IDã®å–å¾—
        row_lines = row.page_content.split("\n")
        row_dict = {item.split(": ")[0]: item.split(": ")[1] for item in row_lines}
        employee_id = row_dict["å¾“æ¥­å“¡ID"]

        doc = ""

        # å–å¾—ã—ãŸå¾“æ¥­å“¡IDã«ç´ã¥ãå•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã‚’å–å¾—
        same_employee_inquiries = []
        for row_history in docs_history:
            row_history_lines = row_history.page_content.split("\n")
            row_history_dict = {item.split(": ")[0]: item.split(": ")[1] for item in row_history_lines}
            if row_history_dict["å¾“æ¥­å“¡ID"] == employee_id:
                same_employee_inquiries.append(row_history_dict)

        new_doc = Document()

        if same_employee_inquiries:
            # å¾“æ¥­å“¡æƒ…å ±ã¨å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
            doc += "ã€å¾“æ¥­å“¡æƒ…å ±ã€‘\n"
            row_data = "\n".join(row_lines)
            doc += row_data + "\n=================================\n"
            doc += "ã€ã“ã®å¾“æ¥­å“¡ã®å•ã„åˆã‚ã›å¯¾å¿œå±¥æ­´ã€‘\n"
            for inquiry_dict in same_employee_inquiries:
                for key, value in inquiry_dict.items():
                    doc += f"{key}: {value}\n"
                doc += "---------------\n"
            new_doc.page_content = doc
        else:
            new_doc.page_content = row.page_content
        new_doc.metadata = {}

        docs_all.append(new_doc)
    
    return docs_all



def get_target_employees(employees, employee_ids):
    """
    å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§ã®å–å¾—

    Args:
        employees: å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§
        employee_ids: å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒã€Œç‰¹ã«ã€é«˜ã„å¾“æ¥­å“¡ã®IDä¸€è¦§

    Returns:
        å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒã€Œç‰¹ã«ã€é«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§
    """

    target_employees = []
    duplicate_check = []
    target_text = "å¾“æ¥­å“¡ID"
    for employee in employees:
        # å¾“æ¥­å“¡IDã®å–å¾—
        num = employee.page_content.find(target_text)
        employee_id = employee.page_content[num+len(target_text)+2:].split("\n")[0]
        # å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ã‚’ã€IDã§ç…§åˆã—ã¦å–å¾—ï¼ˆé‡è¤‡é™¤å»ï¼‰
        if employee_id in employee_ids:
            if employee_id in duplicate_check:
                continue
            duplicate_check.append(employee_id)
            target_employees.append(employee)
    
    return target_employees


def get_slack_ids(target_employees):
    """
    SlackIDã®ä¸€è¦§ã‚’å–å¾—

    Args:
        target_employees: å•ã„åˆã‚ã›å†…å®¹ã¨é–¢é€£æ€§ãŒé«˜ã„å¾“æ¥­å“¡æƒ…å ±ä¸€è¦§

    Returns:
        SlackIDã®ä¸€è¦§
    """

    target_text = "SlackID"
    slack_ids = []
    for employee in target_employees:
        num = employee.page_content.find(target_text)
        slack_id = employee.page_content[num+len(target_text)+2:].split("\n")[0]
        slack_ids.append(slack_id)
    
    return slack_ids


def create_slack_id_text(slack_ids):
    """
    SlackIDã®ä¸€è¦§ã‚’å–å¾—

    Args:
        slack_ids: SlackIDã®ä¸€è¦§

    Returns:
        SlackIDã‚’ã€Œã¨ã€ã§ç¹‹ã„ã ãƒ†ã‚­ã‚¹ãƒˆ
    """
    slack_id_text = ""
    for i, id in enumerate(slack_ids):
        slack_id_text += f"ã€Œ{id}ã€"
        # æœ€å¾Œã®SlackIDä»¥å¤–ã€é€£çµå¾Œã«ã€Œã¨ã€ã‚’è¿½åŠ 
        if not i == len(slack_ids)-1:
            slack_id_text += "ã¨"
    
    return slack_id_text


def get_context(docs):
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€ãŸã‚ã®å¾“æ¥­å“¡æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆ
    Args:
        docs: å¾“æ¥­å“¡æƒ…å ±ã®ä¸€è¦§

    Returns:
        ç”Ÿæˆã—ãŸå¾“æ¥­å“¡æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆ
    """

    context = ""
    for i, doc in enumerate(docs, start=1):
        context += "===========================================================\n"
        context += f"{i}äººç›®ã®å¾“æ¥­å“¡æƒ…å ±\n"
        context += "===========================================================\n"
        context += doc.page_content + "\n\n"

    return context


def get_datetime():
    """
    ç¾åœ¨æ—¥æ™‚ã‚’å–å¾—

    Returns:
        ç¾åœ¨æ—¥æ™‚
    """

    dt_now = datetime.datetime.now()
    now_datetime = dt_now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')

    return now_datetime


def preprocess_func(text):
    """
    å½¢æ…‹ç´ è§£æã«ã‚ˆã‚‹æ—¥æœ¬èªã®å˜èªåˆ†å‰²
    Args:
        text: å˜èªåˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        å˜èªåˆ†å‰²ã‚’å®Ÿæ–½å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
    """

    tokenizer_obj = dictionary.Dictionary(dict="full").create()
    mode = tokenizer.Tokenizer.SplitMode.A
    tokens = tokenizer_obj.tokenize(text ,mode)
    words = [token.surface() for token in tokens]
    words = list(set(words))

    return words


def adjust_string(s):
    """
    Windowsç’°å¢ƒã§RAGãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã‚ˆã†èª¿æ•´
    
    Args:
        s: èª¿æ•´ã‚’è¡Œã†æ–‡å­—åˆ—
    
    Returns:
        èª¿æ•´ã‚’è¡Œã£ãŸæ–‡å­—åˆ—
    """
    # èª¿æ•´å¯¾è±¡ã¯æ–‡å­—åˆ—ã®ã¿
    if type(s) is not str:
        return s

    # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
    if sys.platform.startswith("win"):
        s = unicodedata.normalize('NFC', s)
        s = s.encode("cp932", "ignore").decode("cp932")
        return s
    
    # OSãŒWindowsä»¥å¤–ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    return s

def debug_retriever_output(query, retriever):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã€retrieverãŒè¿”ã™chunkã¨scoreã‚’è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒãƒƒã‚°ç”¨é–¢æ•°

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
        retriever: LangChainã®retrieverã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    Returns:
        ãªã—ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼‰
    """
    print("\n" + "=" * 80)
    print(f"ğŸ” è³ªå•: {query}")
    print("=" * 80)
    
    try:
        results = retriever.vectorstore.similarity_search_with_score(query, k=5)
        for i, (doc, score) in enumerate(results):
            print(f"[{i+1}] Score: {score:.4f}")
            print(f"Chunk Preview: {doc.page_content[:200]}...\n")
    except Exception as e:
        print("âŒ ã‚¹ã‚³ã‚¢ä»˜ãå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
        print("ğŸ” fallback: get_relevant_documents() ã§å‡ºåŠ›ã‚’è©¦ã¿ã¾ã™")
        docs = retriever.get_relevant_documents(query)
        for i, doc in enumerate(docs):
            print(f"[{i+1}] Score: N/A")
            print(f"Chunk Preview: {doc.page_content[:200]}...\n")

def create_retriever(db_name):
    """
    æŒ‡å®šã•ã‚ŒãŸDBãƒ‘ã‚¹ã«åŸºã¥ã„ã¦Retrieverã®ã¿ã‚’ä½œæˆ

    Args:
        db_name: ãƒ™ã‚¯ãƒˆãƒ«DBã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼ˆã¾ãŸã¯å®šç¾©åï¼‰

    Returns:
        LangChainã®Retrieverã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    logger = logging.getLogger(ct.LOGGER_NAME)

    docs_all = []
    if db_name == ct.DB_ALL_PATH:
        folders = os.listdir(ct.RAG_TOP_FOLDER_PATH)
        for folder_path in folders:
            if folder_path.startswith("."):
                continue
            add_docs(f"{ct.RAG_TOP_FOLDER_PATH}/{folder_path}", docs_all)
    else:
        folder_path = ct.DB_NAMES[db_name]
        add_docs(folder_path, docs_all)

    for doc in docs_all:
        doc.page_content = adjust_string(doc.page_content)
        for key in doc.metadata:
            doc.metadata[key] = adjust_string(doc.metadata[key])

    text_splitter = CharacterTextSplitter(
        chunk_size=ct.CHUNK_SIZE,
        chunk_overlap=ct.CHUNK_OVERLAP,
        separator="\n",
    )
    splitted_docs = text_splitter.split_documents(docs_all)
    embeddings = OpenAIEmbeddings()

    if os.path.isdir(db_name):
        db = Chroma(persist_directory=".db", embedding_function=embeddings)
    else:
        db = Chroma.from_documents(splitted_docs, embedding=embeddings, persist_directory=".db")

    retriever = db.as_retriever(search_kwargs={"k": ct.TOP_K})
    return retriever
